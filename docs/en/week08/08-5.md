---
lang-ref: ch.08-2
lecturer: Yann LeCun
title: Regularized Latent Variable Energy Based Models
authors: Henry Steinitz, Rutvi Malaviya, Aathira Manoj
date: 23 Mar 2020
---


## Visual Representation Learning

Representation learning trains a system to produce the representations required for feature detection or classification from raw data. Visual representation learning is about the representations of images or videos in particular.

<image>

This can be broadly classified as shown above and the focus of the lecture would be on the self supervised visual representation learning. 


## Self-supervised visual representation learning:

It is a two stage process comprising pretrainiing and evaluation

##### Step1: Pretraining

Uses a large amount of unlabeled data to train a backbone network. Different methods will produce the backbone network differently

##### Step2: Evaluation

It can be performed in two ways: feature extraction and finetuning. Both these methods generate representation from ​​the image and then use it to train DsTH ( Downstream Task Head ). The learning of the downstream task would thus be in the repsretantion space instead of the image space. The only difference between the two methods is the stop gradient before encoder. In finetuning, we can change the encoder unlike in feature extraction. 

<image>


### Generative

The popular one is denoising autoencoder. You train the model to reconstruct the original image from the noisy image. After the training, we retain the encoder for the downstream task.

##### Issues:
 
The model tries to solve a problem that is too hard. For example: for a lot of downstream tasks, you don't really have to reconstruct the image.
Also, sometimes the loss function is not good enough. For ex: the eucliean distance used as a reconstruction loss metric isn’t be a good metric for comparing the similarity between two images.

<image>

### Pretext Task

Pretext task:
<insert image>
It’s almost the same as above but you train the model to figure out a smart way to generate pseudo labels. For ex: Given the image of a tiger, the shuffled image is the input x, the output y would be the correct way of labeleing the patches. The network successfully reinventing the patches indicates that this understands the image. 

##### Issues:
Designing the pretext task is tricky. if you design your protect us too easy, the network, won’t learn really good representation.But if they are designed really hard, it can become harder than the downstream task and the network isn't really trained very well. Also, the representations generated via this method will be tailored to the specific downstream task.

<image>

### Joint-embedding methods

Joint embedding methods perform better than the above two methods. These have been discussed in great detail in this lecture.


#### Improvisations for JEMs

We can further improve these models by experimenting with data augmentation and network architecture. We don’t have really good understanding of these but they are very important - finding good augmentation may boost more performance than changing loss function. 

##### Data Augmentation

Most dominant augmentations were proposed by simCLR and improved a little bit by BYOL:
1. Random Crop (the most critical one)
2. Flip
3. Color Jitter
4. Gaussian Blur

It has been found empirically that random crop is the most critical one. It might be because the random crop is the only we can change the spatial information about the images. Flip does the same partly but is really week. Color jitter and gaussian blur change channels. 

( add image )

##### Masking augmentation:
Recently people are moving towards masking augmentation instead of traditional augmentation in which we mask out most ( ~75% in the below image ) of the patches. It can replace random crop since it’s another way to remove redundancy of the spatial information
( add image )

Issues:
This works well only with transformer type of architecture and not with convnet. This is because masking introduces too many random artificial edges. For any transformer, the first layer is the conv layer, with kernel size equal to the patch size and thus, this never experiences artifical edges. For convnets which have sliding windows, the artifical edges can't be ignored and will result in noise.

#### Network Architecture

##### Projector/Expander:
It is a two/three-layer feed forward neural network and empirical results show that it is always better to add this in the network architecture. 

Projector is used to project into lower dimension and expander is used to project in higher dimension. Projector is used only during the pretraining and removed while performing the downstream task. This is because projector removes a lot of information even if the output dimension of the projector and the backbone is same. 

##### Momentum Encoder:
Even without memory bank, momentum encoder usually helps the performance of the downstream tasks, especially with weak data augmentation. 

( add image )

