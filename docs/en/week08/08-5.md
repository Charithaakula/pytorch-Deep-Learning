---
lang-ref: ch.08-5
lecturer: Alfredo Canziani and Jiachen Zhu
title: Visual Representation Learning
authors: Sai Charitha Akula
date: 12 May 2022
---


## Visual Representation Learning

Representation learning trains a system to produce the representations required for feature detection or classification from raw data. Visual representation learning is about the representations of images or videos in particular.

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig1.png" weight="75%"/><br>
<b>Fig. 1</b>: Visual Representation Learning
</center>

This can be broadly classified as shown above and the focus of the lecture would be on self-supervised visual representation learning. 


## Self-supervised Visual Representation Learning

It is a two stage process comprising pretraining and evaluation

##### Step1: Pretraining

Uses a large amount of unlabeled data to train a backbone network. Different methods will produce the backbone network differently

##### Step2: Evaluation

It can be performed in two ways: feature extraction and finetuning. Both these methods generate representation from ​​the image and then use it to train DsTH ( Downstream Task Head ). The learning of the downstream task would thus be in the representation space instead of the image space. The only difference between the two methods is the stop gradient before the encoder. In finetuning, we can change the encoder unlike in feature extraction. 

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig2.png" weight="40%"/><br>
<b>Fig. 2</b>: Self-supervised Visual Representation Learning
</center>

### Generative Models

The popular one is the denoising autoencoder. You train the model to reconstruct the original image from the noisy image. After the training, we retain the encoder for the downstream task.

##### Issues:
 
The model tries to solve a problem that is too hard. For example: For a lot of downstream tasks, you don't have to reconstruct the image, which is a tougher problem than the downstream task itself. Also, sometimes the loss function is not good enough. For example: the Euclidean distance used as a reconstruction loss metric isn’t a good metric for comparing the similarity between two images.

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig3.png" height="75%" weight="75%"/><br>
<b>Fig. 3</b>: Generative Models - Autoencoder
</center>

### Pretext Tasks

It’s almost the same as above but you train the model to figure out a smart way to generate pseudo labels. For example: Given the image of a tiger, the shuffled image is the input x, and the output y would be the correct way of labeling the patches. The network successfully reinventing the patches indicates that it understands the image. 

##### Issues:
Designing the pretext task is tricky. if you design the task too easy, the network won’t learn good representation. But if you design the task hard, it can become harder than the downstream task and the network wouldn't be trained well. Also, the representations generated via this method will be tailored to the specific downstream task.

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig4.png" height="75%" weight="75%"/><br>
<b>Fig. 4</b>: Pretext Tasks
</center>

### Joint-embedding methods

Joint embedding methods perform better than the above two methods. These have been discussed in great detail in this [lecture](https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-4/).


#### Improvisations for JEMs

We can further improve these models by experimenting with data augmentation and network architecture. We don’t have a good understanding of these but they are very important. In fact, finding good augmentation may boost more performance than changing the loss function. 

##### Data Augmentation

Most dominant augmentations were proposed by simCLR and improved a little bit by BYOL:
1. Random Crop (the most critical one)
2. Flip
3. Color Jitter
4. Gaussian Blur

It has been found empirically that random crop is the most critical one. It might be because the random crop is the only we can change the spatial information about the images. Flip does the same partly but is weak. Color jitter and gaussian blur change channels. 

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig5.png" height="25%" /><br>
<b>Fig. 5</b>: Data Augmentation
</center>

##### Masking augmentation:
Recently people are moving towards masking augmentation instead of traditional augmentation in which we mask out most ( ~75% in the below image ) of the patches. It can replace random crop since it’s another way to remove the redundancy of the spatial information

**Issues:**
This works well only with transformer type of architecture and not with convnet. This is because masking introduces too many random artificial edges. For any transformer, the first layer is the conv layer, with kernel size equal to the patch size and thus, this never experiences artificial edges. For convnets which have sliding windows, the artificial edges can't be ignored and will result in noise.

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig6.png" weight="50%" /><br>
<b>Fig. 6</b>: Masked Augmentation
</center>

#### Network Architecture

##### Projector/Expander:
It is a two/three-layer feed-forward neural network and empirical results show that it is always better to add this in the network architecture. 

The projector is used to project into a lower dimension and the expander is used to project into a higher dimension. A projector is used only during the pretraining and removed while performing the downstream task. This is because the projector removes a lot of information even if the output dimension of the projector and the backbone are the same. 

##### Momentum Encoder:
Even without a memory bank, a momentum encoder usually helps the performance of the downstream tasks, especially with weak data augmentation. 

<center>
<img src="{{site.baseurl}}/images/week08/08-5/5_fig7.png" weight="50%" /><br>
<b>Fig. 7</b>: Projector/Expander
</center>

