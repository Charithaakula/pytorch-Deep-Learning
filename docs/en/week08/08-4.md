---
lang-ref: ch.08-2
lecturer: Yann LeCun
title: Regularized Latent Variable Energy Based Models
authors: Henry Steinitz, Rutvi Malaviya, Aathira Manoj
date: 23 Mar 2020
---


## Joint Embedding Methods

As mentioned by Dr.Lecun, we can broadly classify Energy Based Models in the following ways based on training methods and architectures for multimodal prediction


| EBMS       |  Training Methods          | Architectures  |
| ------------- |:-------------:| :-----:|
| 1.      | Contrastive methods | Latent variable models |
| 2.      | Regularized & Architectural methods      |   Latent variable models |
| 3.      | Contrastive methods | Joint embedding architectures |
| 4.      | Regularized & Architectural methods      | Joint embedding architectures |


[comment]: <>(1. Contrastive methods, Latent variable models)
[comment]: <>(2. Regularized & Architectural methods, Latent variable models)
[comment]: <>(3. Contrastive methods, Joint embedding architectures)
[comment]: <>(4. Regularized & Architectural methods, Joint embedding architectures)

In this lecture, Jiachen gave a broad overview of Joint embedding architectures and various training methods that can be used. 

### Introduction

Joint Embedding methods try to make their backbone network robust to certain distortions and are invariant to data augmentation. 

As an example, as shown in the image below, for an image of dog, you take two distorted versions of the image, then encode them with your backbone network to represetnations and you want them to be close to each other. Thus ensuring the two images share some semantic information.
<inseert image>

They also prevent trivial solutions. The network could collapse with just above condition, as the network can become invariant not only to distortions but to the input altogether i.e., irrespective of the input, it could generate the same output. JEMs try to prevent this trivial solution in different ways.

Instead of consdiering only local energy ( between two pairs of distorted images ), this actually gets a batch of the images and ensures the collection of the representation, Hx, doesn’t have same rows or columns. ( which is the trivial solution )

<image>

[comment]: <>( D is the energy that is calclualated per sample. A and B are the loss  functionals that are calculated per batch of size N. The dotted operator is for stacking. hx, hy are the representation of x, y and Hx nad Hy are the matrixes with each row as hx.)

### Components:

Every Joint Embedding Method has the following components:

1. Data augmentation ( x and y ): The way you generate the two distorted versions of the image.
2. Backbone Network ( BB ) - The definition of the backbone
3. Energy function ( D ) - The definition of the distance between the two representations.
4. Loss functionals ( A and B ) - The definition of the loss functionals calculated per batch of size N.

### Joint Embedding Loss Functions:

Joint Embedding Loss Functions contain two components:
1. A term that pushes the positive pair closer
2. An (implicit) term that prevents the trivial solution (constant output) - implict because a lot of "other methods" do not have an explicit term to prevent the trivial solution from happening.

To make the training stable, people usually normalize the embeddings or put a hinge on the loss function to prevent the norm of embeddings becoming too large or too small

### Training Methods

The training methods can be further classified into the following four types:
1. Contrastive methods
2. Non-Contrastive methods
3. Clustering methods
4. Other methods

We now go into the details of each of these methods

### Contrastive methods

Contrastive methods push positive pairs closer and negative pairs away. More details about the contrastive methods including MoCo, PIRL and SimCLR have been discussed [here](https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-1/). 

#### The infoNCE loss function:
Both SimCLR and MoCO use the InfoNCE loss function.
( insert image of infonce )
Hjx - repreesetnation from other images

The first term indicates the similarity between postiive pairs and the second term is the softmax between all the negative pairs. We would like to minimize this whole function.

Notice that it gives different weights to different negative samples. The negative pair that has high similarity is pushed much harder than the negative pair with low similarity because there's a soft Max. Also, the similarity measurement here is the  the inner product between the two representations and to prevent the gradient explosion, the norm is normalized. Thus, even if the vector grew really long, the term ensures that it is a unit vector.

#### Memory Bank:

As already mentioned, these models require negative samples. However, finding negative pairs becomes difficult as the embedding spaces become large. 

To handle this, SimCLR and MoCO use large batch size to find the samples. The difference between SimCLR and MoCO is how we deal with the large batch size. SimCLR uses 8192 as the batch size. However, to solve the requirement of large batch size without actually using a large batch size, MoCO uses memory bank. We use a small batch size but instead of using negative samples from only this batch, you collect them even from previous batches. For ex: with a 256 batch size, aggregating previous 32 batches of negative samples results essentially in a batch size of 8192. This method saves memory and avoids the effort to generate the negative samples again and again.

(first imageof mb )

Issue:
Because B is updated every step, the back bone is updated every step, and thus, after a while, the old negative samples are not valid anymore and can lead to decrease in performance. To avoid this, MOCO uses momentum backbone that slows down the training of the right backbone. In that case the difference between the older momentum backbone, and the new momentum backbone is not that different so it means those a negative sample is still valid even you after a while.

(second imageof mb )

Vt+1 ( momemtum backbone’s parameter ) is an exponential moving average of theta t. When m is large like 0.99, the learning rate of vt is 100 times smaller than learning rate of theta t. In geenral, it is ( 1 -  m )* eta. High values of m will make the vt stable. If m is very small like 0, vt+1 is theta t+1. M =1 will make vt basically untraiend.

( just the equations image )

#### Disadvantages of Contrastive methods:

In practise, people found out that ocntrastive methods need a lot of setup to make it work. They require techniques such as weight sharing between the branches, batch normalization, feature-wise normaliation, output quantization, stop gradient, memory banks etc.,. This makes it really hard to analyse. Also, it is not stable if you don’t use those techniques. 

### Non-Contrastive methods

#### Non-Contrastive methods and information theory:

Most of the non-contrastive methods are based on information theory.  Ex: Redundancy reduction ( Barlow Twins ) and Information.  They dont’ require special architectures or engineering techniques. 

#### VicReg:

It tries to maximize the information content of the emebeddings by producing embedding variables that are decoorelated from each other. If the variables are correlated to each other, they covariate together and the information content is reduced. Thus, it prevents an informational collapse in which the variables carry redundant information. Also, this method requires comparativiely small batch size.

Two types of collapse can occur in these architectures:
Type 1: Irrepostive of what you send, the network generates same representation
Type 2: Special colapose - Although different image have different representation, the information content is really low in each representation.    

##### Loss function:
The loss function is pushing:
1. Positive pairs closer - to be invariant to data augmentation
2. The variance of the emebddings large by pushing all of the diagonal terms of the covariance matrix large - to prevent first kind of collapse
3. The covraiance of the embeddings small by pushing all off the diagonal terms of the covariance matrix small- to prevent second kind of collapse.

(iimage) 

### Clustering methods

#### SwAV

This method prevents trivial solution by quantizing the embedding space. SwAV does the following:

1. Generates representations and stack the generated representations ( into Hx and Hy ).
2. Applies sinkhorn clustering method to each of the stacked representation to generate corresponding clustered Q matrices where each row ( qx )  represents a one hot vector indicating the cluster the corresponding representation belongs to
3.Performs second clustering for the representations hx and hy with soft-kmeans and with the same centroid ( W belong to R K by  D ) . This step generates predictions for Qx and Qy, qx~ and qy~, from hy and hx respectively ( Thus, called swap prediction )
4.Minimize the loss function which is the sum of two crossentropy functions between qx and qx~ and qy and qy~.

##### The Loss function:

Sinkhorn algorithm:
Sinkhorn algorithm can distribute samples to not just one cluster but to every cluster. Thus, it can help us prevent all the data clustering into a single centroid or any such non uniform distribution. It takes in hyperparameters that allow us to deploy different levels of uniform distribution across clusters degenerating to K-means algorithm on one extreme and to perfectly uniform distribution on the other extreme

Softargmax clustering:
Each hy is normalized. W*hy indicates similarity between hy and all other centroids. Softargmax turns the cosine similarly ( positive or negative ) into a probability. 

Since this is predicting the qx, we will compare the cross entropy of the prediction, qx~, with the actual qx to measure the prediction. 
( add image )

##### Interpretation of clusters:
This method partitions latent space into a few clusters automatically without labels and the hope is that these clusters will be related to the actual clases. Thus, later, we would just need few labeled data samples to assign each cluster to the corresponding label under supervised learning.

##### Invariance to data augmentation:
Instead of pushing the pairs closer to each other, you push both the representations to be inside the same cluster. 

##### Preventing trivial solution
In trivial solution, all the representations will be same and thus belong to the same centroid. However, with sinkhorn, different clusters have equal number of samples, thus the representations can’t be put into one centroid, preventing trivial solution.

### Other methods

The loss function for all the previous methods including contrasting methods need a batch or pool of negative samples, thus creating problems with distributed training. However, the loss functions of these methods are local. These methods perform well but an understanding of why they don’t collapse is not yet available. Probably there's some implicit regularization happening in these networks to prevent them from converging to a trivial solution. ß

(images for all )

#### BYOL:
BOYL adds a predictor, predicting hy from hx. The energy function ( D ) is a cosine similarity between hy and predicted hy. There is no term for negative samples i.e., this method only pushes positive pairs closer and has no enforcement on negative pairs. It is thought that asymmetrical architecture with extra layers makes this method work. 

SimSiam is a folloup version which uses regular backbone instead of the momentum back bone

#### Dino:
The two softargmax components used have different coldness or temperature. The energy function is the cross entropy between these two, pushing them together. Even this method doesn’t enforce anything on negative samples.

#### Data2Vec:
Adds a layer norm at the end of the representation.

##### Initialization of the network:
If you initialize the network with trivial solution, then that network will never work.  because if trivial solution is already present, the loss function will produce a zero gradient, thus can never escape from the trivial solution. However, in other cases, the training dynamic adjusted in a way that they never converge in these methods.


