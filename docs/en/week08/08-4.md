---
lang-ref: ch.08-4
lecturer: Alfredo Canziani and Jiachen Zhu
title: Joint Embedding Methods
authors: Sai Charitha Akula
date: 12 May 2022
---


## Joint Embedding Methods

As mentioned by Dr.Lecun, we can broadly classify Energy Based Models in the following ways based on training methods and architectures for multimodal prediction


| EBMS       |  Training Methods          | Architectures  |
| ------------- |:-------------:| :-----:|
| 1.      | Contrastive methods | Latent variable models |
| 2.      | Regularized & Architectural methods      |   Latent variable models |
| 3.      | Contrastive methods | Joint embedding architectures |
| 4.      | Regularized & Architectural methods      | Joint embedding architectures |
<!---
[comment]: <>(1. Contrastive methods, Latent variable models)
[comment]: <>(2. Regularized & Architectural methods, Latent variable models)
[comment]: <>(3. Contrastive methods, Joint embedding architectures)
[comment]: <>(4. Regularized & Architectural methods, Joint embedding architectures)
---->
In this lecture, Jiachen gave a broad overview of Joint embedding architectures and various training methods that can be used. 

### Introduction

Joint Embedding methods try to make their backbone network robust to certain distortions and are invariant to data augmentation. 

As an example, as shown in the image below, for an image of a dog, you take two distorted versions of the image, then encode them with your backbone network to generate representations and you make them to be close to each other. Thus, ensuring the two images share some semantic information.

<center>
<img src="{{site.baseurl}}/images/week08/08-4/4_fig0.png" weight="50%"/><br>
<b>Fig. 1</b>: Data Augmentation in JEM
</center>

They also prevent trivial solutions. The network could collapse with just the above condition, as the network can become invariant not only to distortions but to the input altogether i.e., irrespective of the input, it could generate the same output. JEMs try to prevent this trivial solution in different ways.

Instead of considering only local energy ( between two pairs of distorted images ), these methods get a batch of the images and ensure that the collection of the representation, $H_{x}$, doesn’t have the same rows or columns. ( which is the trivial solution )

<center>
<img src="{{site.baseurl}}/images/week08/08-4/4_fig1.png" weight="50%"/><br>
<b>Fig. 2</b>: Preventing Trivial Solutions in JEM
</center>
<!---
[comment]: <>( D is the energy that is calculated per sample. A and B are the loss  functionals that are calculated per batch of size N. The dotted operator is for stacking. hx, hy are the representation of x, y and Hx nad Hy are the matrixes with each row as hx.)
---->

### Components:

Every Joint Embedding Method has the following components:

1. Data augmentation ( x and y ): The way you generate the two distorted versions of the image.
2. Backbone Network ( BB ) - The definition of the backbone
3. Energy function ( D ) - The definition of the distance between the two representations.
4. Loss functionals ( A and B ) - The definition of the loss functionals calculated per batch of size N.

### Joint Embedding Loss Functions:

Joint Embedding Loss Functions contain two components:
1. A term that pushes the positive pair closer
2. An (implicit) term that prevents the trivial solution (constant output) - implicit because a lot of "other methods" do not have an explicit term to prevent the trivial solution.

To make the training stable, people usually normalize the embeddings or put a hinge on the loss function to prevent the norm of embeddings from becoming too large or too small

### Training Methods

The training methods can be further classified into the following four types:
1. Contrastive methods
2. Non-Contrastive methods
3. Clustering methods
4. Other methods

We now go into the details of each of these methods

### Contrastive methods

Contrastive methods push positive pairs closer and negative pairs away. More details about the contrastive methods including MoCo, PIRL, and SimCLR have been discussed [here](https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-1/). 

#### The InfoNCE loss function:
Both SimCLR and MoCO use the InfoNCE loss function.
<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig2.png" height="75%" width="75%"/><br>
<b>Fig. 3</b>: InfoNCE Loss Function
</center>

The first term indicates the similarity between positive pairs and the second term is the softmax between all the negative pairs. We would like to minimize this whole function.

Notice that it gives different weights to different negative samples. The negative pair that has high similarity is pushed much harder than the negative pair with low similarity because there's a softmax. Also, the similarity measurement here is the inner product between the two representations, and to prevent the gradient explosion, the norm is normalized. Thus, even if the vector grew long, the term ensures that it is a unit vector.

#### Memory Bank:

As already mentioned, these models require negative samples. However, finding negative pairs becomes difficult as the embedding spaces become large. 

To handle this, SimCLR and MoCO use large batch sizes to find the samples. The difference between SimCLR and MoCO is the way they deal with the large batch size. SimCLR uses 8192 as the batch size. However, MoCO tries to solve the requirement of a large batch size without actually using a large batch size by using a memory bank. It uses a small batch size but instead of using negative samples from only the current batch, it collects them even from previous batches. For example: with a 256 batch size, aggregating the previous 32 batches of negative samples results essentially in a batch size of 8192. This method saves memory and avoids the effort to generate the negative samples again and again.

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig3.png" height="75%" width="75%"/><br>
<b>Fig. 4</b>: Memory Bank
</center>

Issue:
Because B is updated every step, the backbone is updated every step, and thus, after a while, the old negative samples are not valid anymore and can lead to a decrease in performance. To avoid this, MoCO uses a momentum backbone that slows down the training of the right backbone. In that case, the difference between the older momentum backbone and the new momentum backbone is not that different, retaining the validitiy of the negative sample even after a while.

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig4.png" height="75%" width="75%"/><br>
<b>Fig. 5</b>: Memory Bank with Momentum Backbone
</center>

$$\vartheta_{t+1}$$ ( momemtum backbone’s parameter ) is an exponential moving average of $$\Theta_{t}$$. The learning rate of $$\vartheta$$ is $$( 1 -  m )* \eta$$. High values of m will make the $$\vartheta_{t}$$ stable. m =1 will make $$\vartheta_{t}$$  basically untrained. If m is very small like 0, $$\vartheta_{t+1}$$ is $$\Theta_{t+1}$$. 

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig5.png" height="25%" width="50%"/><br>
<b>Fig. 6</b>: Exponential Moving Average
</center>

#### Disadvantages of Contrastive methods:

In practice, people found out that contrastive methods need a lot of setup to make them work. They require techniques such as weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks etc.,.This makes it hard to analyze. Also, they are not stable without the use of those techniques. 

### Non-Contrastive methods

#### Non-Contrastive methods and information theory:

Most of the non-contrastive methods are based on information theory. For example: Redundancy reduction ( Barlow Twins ) and Information.  They don't require special architectures or engineering techniques. 

#### VicReg:
It tries to maximize the information content of the embeddings by producing embedding variables that are decorrelated to each other. If the variables are correlated to each other, they covariate together and the information content is reduced. Thus, it prevents an informational collapse in which the variables carry redundant information. Also, this method requires a comparatively small batch size.

Two types of collapse can occur in these architectures:
Type 1: Irrespective of the input, the network generates the same representation
Type 2: Special collapse - Although different images have different representations, the information content is really low in each representation.       

##### Loss function:
The loss function is pushing:
1. Positive pairs closer - to be invariant to data augmentation
2. The variance of the embeddings large by pushing all of the diagonal terms of the covariance matrix large - to prevent the first kind of collapse
3. The covariance of the embeddings small by pushing all off the diagonal terms of the covariance matrix small- to prevent the second kind of collapse.

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig6.png" width="50%"/><br>
<b>Fig. 7</b>: VicReg Loss Function
</center>

### Clustering methods

#### SwAV

This method prevents trivial solution by quantizing the embedding space. SwAV does the following:

1. Generates representations and stack the generated representations ( into $$H_{x}$$ and $$H_{y}$$ ).
2. Applies sinkhorn clustering method to each of the stacked representation to generate corresponding clustered Q matrices where each row ( $$q_{x}$$ ) represents a one hot vector indicating the cluster the corresponding representation belongs to
3.Performs second clustering for the representations $$h_{x}$$ and $$h_{y}$$ with soft-kmeans. This step generates predictions for $$q_{x}$$ and $$q_{y}$$, $$\tilde{q_{x}}$$ and $$\tilde{q_{y}}$$, from $$h_{y}$$ and $$h_{x}$$ respectively ( Thus, called swap prediction )
4.Minimizes the loss function which is the sum of two crossentropy functions between $$q_{x}$$ and $$\tilde{q_{x}}$$ and $$q_{y}$$ and $$\tilde{q_{y}}$$.

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig7.png" height="75%" width="75%"/><br>
<b>Fig. 8</b>: SWaV
</center>

##### The Loss function:

Sinkhorn algorithm:
Sinkhorn algorithm can distribute samples to not just one cluster but to every cluster. Thus, it can help us prevent all the data clustering into a single centroid or any such nonuniform distribution. It takes in hyperparameters that allow us to deploy different levels of uniform distribution across clusters degenerating to K-means algorithm on one extreme and to the perfectly uniform distribution on the other extreme

Softargmax clustering:
Each $$h_{y}$$ is normalized. W*$$h_{y}$$ indicates similarity between $$h_{y}$$ and all other centroids. Softargmax turns the cosine similarly ( positive or negative ) into a probability. 

Since this is predicting the $$q_{x}$$, we will compare the cross entropy of the prediction, $$\tilde{q_{x}}$$, with the actual $$q_{x}$$ to measure the prediction. 

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig8.png" width="50%"/><br>
<b>Fig. 9</b>: SWaV Loss Function
</center>

##### Interpretation of clusters:
This method partitions latent space into a few clusters automatically without labels and the hope is that these clusters will be related to the actual classes. Thus, later, we would just need a few labeled data samples to assign each cluster to the corresponding label under supervised learning.

##### Invariance to data augmentation:
Instead of pushing the pairs closer to each other, you push both the representations to be inside the same cluster. 

##### Preventing trivial solution
In a trivial solution, all the representations will be the same and thus belong to the same centroid. However, with sinkhorn, different clusters have an equal number of samples, thus the representations can’t be put into one centroid, preventing a trivial solution.

### Other methods

The loss function for all the previous methods including contrasting methods needs a batch or pool of negative samples, thus creating problems with distributed training. However, the loss functions of these methods are local. These methods perform well but an understanding of why they don’t collapse is not yet available. Probably there's some implicit regularization happening in these networks to prevent them from converging to a trivial solution. 

<center>
<img src="{{site.baseurl}}/images/week08/08-4/fig9.png" height="75%" width="75%"/><br>
<b>Fig. 10</b>: Other Methods
</center>

#### BYOL:
BOYL adds a predictor, predicting $$h_{y}$$ from $$h_{x}$$. The energy function ( D ) is a cosine similarity between $$h_{y}$$ and predicted $$h_{y}$$. There is no term for negative samples i.e., this method only pushes positive pairs closer and has no enforcement on negative pairs. It is thought that asymmetrical architecture with extra layers makes this method work. 

SimSiam is a followup version that uses a regular backbone instead of the momentum backbone

#### Dino:
The two softargmax components used have different coldness or temperature. The energy function is the cross entropy between these two, pushing them together. Even this method doesn’t enforce anything on negative samples.

#### Data2Vec:
Adds a layer norm at the end of the representation.

##### Initialization of the network:
If you initialize the network with a trivial solution, then that network will never work.  This is because if the trivial solution is already achieved, the loss function will produce a zero gradient and thus, can never escape from the trivial solution. However, in other cases, the training dynamic is adjusted in a way that they never converge in these methods.

