---
lang-ref: ch.08-2
lecturer: Yann LeCun
title: Regularized Latent Variable Energy Based Models
authors: Henry Steinitz, Rutvi Malaviya, Aathira Manoj
date: 23 Mar 2020
---


## Joint Embedding Methods

As mentioned by Dr.Lecun, we can broadly classify Energy Based Models in the following ways based on training methods and architectures for multimodal prediction

1. Contrastive methods, Latent variable models
2. Regularized & Architectural methods, Latent variable models
3. Contrastive methods, Joint embedding architectures
4. Regularized & Architectural methods, Joint embedding architectures

In this lecture, Jiachen gave a broad overview of Joint embedding architectures and various training methods that can be used. 

### Intro

Joint embedding methods are invariant to data augmentation and prevent trivial solutions. He classifies Joint Embedding methods into four types:

1. Contrastive methods
2. Non-Contrastive methods
3. Clustering methods
4. Other methods

We now go into the details of each of these methods

### Contrastive methods

More details about these can be found [here](https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-1/)

### Non-Contrastive methods

### Clustering methods

Sparse coding is an example of an unconditional regularized latent-variable EBM which essentially attempts to approximate the data with a piecewise linear function.

$$E(z, y) = \Vert y - Wz\Vert^2 + \lambda \Vert z\Vert_{L^1}$$

The $n$-dimensional vector $z$ will tend to have a maximum number of non-zero components $m << n$. Then each $Wz$ will be elements in the span of $m$ columns of $W$.

After each optimization step, the matrix $W$ and latent variable $z$ are normalized by the sum of the $L_2$ norms of the columns of $W$. This ensures that $W$ and $z$ do not diverge to infinity and zero.


### Other methods



